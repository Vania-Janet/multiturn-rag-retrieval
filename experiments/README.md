# Experiment Definitions

This directory contains results for all experimental configurations in our multi-domain RAG evaluation.

## ğŸ“‹ Experiment Overview

| ID | Name | Query Rewrite | Retrieval | Reranking | Purpose |
|----|------|---------------|-----------|-----------|---------|
| **A0** | Baseline Sparse | âŒ | BM25/ELSER/SPLADE | âŒ | Sparse retrieval baseline |
| **A1** | Dense Baseline | âŒ | BGE-M3 | âŒ | Dense retrieval baseline |
| **A2** | Rewrite + Sparse | âœ… Single | SPLADE | âŒ | Query expansion impact |
| **A3** | Rewrite Multi + RRF | âœ… Multi-variant | BM25 | âŒ | Multi-query fusion |
| **A4** | Rewrite + SPLADE | âœ… Single | SPLADE | âŒ | Enhanced sparse with rewrite |
| **A5** | Hybrid Sparse-Dense | âœ… | SPLADE + BGE-M3 | âŒ | RRF fusion of retrievers |
| **A6** | Hybrid + Rerank | âœ… | SPLADE + BGE-M3 | âœ… Cross-encoder | Full pipeline |
| **A7** | Domain-Gated | âœ… | SPLADE + BGE-M3 | âœ… Cross-encoder | A6 + domain rules |
| **A8** | Iterative Refinement | âœ… Iterative | SPLADE + BGE-M3 | âœ… | Multi-round refinement |
| **A9** | ColBERT Rerank | âœ… | SPLADE + BGE-M3 | âœ… ColBERT | Late interaction reranking |
| **A10** | Fine-tuned Reranker | âœ… | SPLADE + BGE-M3 | âœ… FT reranker | Domain-adapted reranker |
| **A11** | Fine-tuned SPLADE | âœ… | FT SPLADE | âŒ | Domain-adapted SPLADE |

## ğŸ¯ Experiment Goals

### Baselines (A0-A1)
Establish lower bounds with standard retrieval methods without any query processing or reranking.

### Query Processing (A2-A4)
Evaluate the impact of query rewriting and expansion techniques on retrieval quality.

### Hybrid Retrieval (A5)
Combine sparse (lexical) and dense (semantic) retrieval using Reciprocal Rank Fusion.

### Reranking (A6, A9-A11)
Apply cross-encoder and late-interaction rerankers to improve final ranking quality.

### Domain Adaptation (A7, A10-A11)
Leverage domain-specific rules and fine-tuned models for specialized performance.

### Advanced Techniques (A8)
Explore iterative query refinement based on initial retrieval results.

## ğŸ“‚ Directory Structure

Each experiment directory follows this structure:

```
A{N}_{experiment_name}/
â”œâ”€â”€ config_resolved.yaml          # Merged configuration (base + domain + experiment)
â”œâ”€â”€ clapnq/                        # Results per domain
â”‚   â”œâ”€â”€ queries/
â”‚   â”‚   â””â”€â”€ expanded.jsonl        # Processed/rewritten queries
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ run.trec              # Retrieval results (TREC format)
â”‚   â”œâ”€â”€ reranking/
â”‚   â”‚   â””â”€â”€ run.trec              # Reranked results (if applicable)
â”‚   â””â”€â”€ eval/
â”‚       â””â”€â”€ metrics.json          # Evaluation metrics
â”œâ”€â”€ fiqa/                          # Same structure for each domain
â”œâ”€â”€ govt/
â”œâ”€â”€ cloud/
â”œâ”€â”€ aggregate/
â”‚   â”œâ”€â”€ metrics_macro.json        # Macro-averaged metrics across domains
â”‚   â””â”€â”€ metrics_table.csv         # Results table for paper
â””â”€â”€ logs/
    â””â”€â”€ run.log                   # Execution logs
```

## ğŸ”„ Configuration System

Experiments are defined purely through YAML configurations (no code changes needed):

1. **Base config** (`configs/base.yaml`): Global defaults
2. **Domain config** (`configs/domains/{domain}.yaml`): Domain-specific parameters
3. **Experiment config** (`configs/experiments/A{N}_{name}.yaml`): Module activation

### Example: Running A6

```bash
python src/pipeline/run.py \
  --experiment A6_hybrid_rerank \
  --domain clapnq
```

This loads:
- `configs/base.yaml`
- `configs/domains/clapnq.yaml`
- `configs/experiments/A6_hybrid_rerank.yaml`

And saves results to `experiments/A6_hybrid_rerank/clapnq/`

## ğŸ“Š Evaluation Metrics

All experiments report:
- **Recall@k** (k=5, 10, 20, 100): Retrieval coverage
- **MRR**: Mean Reciprocal Rank
- **NDCG@k** (k=5, 10, 20): Ranking quality
- **MAP**: Mean Average Precision
- **Latency**: End-to-end query processing time (ms)

## ğŸš« Data Leakage Prevention

- All experiments use **fixed train/val/test splits** defined in `splits/{domain}.yaml`
- Conversation-level splits for conversational datasets (ClapNQ)
- Query-level splits for Q&A datasets (FiQA, Govt, Cloud)
- **Test set is pseudo-test**: used only for final evaluation, never for tuning

## ğŸ“ Reproducibility

Each experiment directory contains:
1. **config_resolved.yaml**: Exact configuration used (for reproduction)
2. **logs/run.log**: Full execution log with timestamps and system info
3. **TREC format outputs**: Standard IR evaluation format

All results can be regenerated by running the pipeline with the saved config.

## ğŸ”— Related Directories

- **configs/**: Configuration definitions
- **indices/**: Pre-built search indices (reused across experiments)
- **models/**: Fine-tuned models (A10, A11)
- **splits/**: Train/val/test split definitions

## ğŸ“„ Paper Reference

Experiment results are aggregated in `aggregate/metrics_table.csv` for direct inclusion in the paper.

Main results table shows:
- Per-domain performance for each experiment
- Macro-averaged performance across domains
- Statistical significance tests between key experiments
