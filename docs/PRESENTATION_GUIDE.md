# GuÃ­a para PresentaciÃ³n: Resultados de Experimentos RAG Multi-dominio

**PropÃ³sito**: Explicar de forma clara y simple los experimentos, metodologÃ­a, ablaciÃ³n y resultados para presentaciÃ³n acadÃ©mica.

---

## ğŸ“Š SLIDE 1: Contexto y Objetivo

### Â¿QuÃ© problema estamos resolviendo?
**Retrieval en conversaciones multi-turno**: Cuando un usuario hace varias preguntas seguidas, el sistema debe entender el contexto de toda la conversaciÃ³n para recuperar documentos relevantes.

### Ejemplo PrÃ¡ctico
```
Usuario: "Â¿CÃ³mo configuro AWS S3?"
Usuario: "Â¿Y los permisos?"  â† Â¡Necesita contexto de la pregunta anterior!
Usuario: "Â¿CuÃ¡l es el costo?"  â† Â¡Necesita saber que hablamos de AWS S3!
```

### Nuestro Objetivo
Evaluar diferentes mÃ©todos de retrieval en **4 dominios diferentes** y encontrar la mejor configuraciÃ³n.

### Dominios Evaluados
1. **ClapNQ**: Preguntas conversacionales generales
2. **FiQA**: Preguntas financieras
3. **Govt**: Documentos gubernamentales
4. **Cloud**: DocumentaciÃ³n tÃ©cnica (AWS, Azure, etc.)

---

## ğŸ§ª SLIDE 2: MetodologÃ­a Experimental (DiseÃ±o Simple)

### Estructura de Experimentos en 3 Fases

```
FASE 1: BASELINES              FASE 2: HÃBRIDOS           FASE 3: RERANKING
(MÃ©todos individuales)         (Combinar lo mejor)        (Refinar top resultados)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BM25      â”‚                â”‚   SPLADE    â”‚            â”‚   Hybrid    â”‚
â”‚   SPLADE    â”‚  â†’             â”‚      +      â”‚  â†’         â”‚      +      â”‚
â”‚   BGE-1.5   â”‚  Evaluar       â”‚   Voyage/   â”‚  Fusionar  â”‚  Reranker   â”‚
â”‚   BGE-M3    â”‚  Individual    â”‚   BGE-1.5   â”‚  RRF       â”‚  (Cross-    â”‚
â”‚   Voyage-3  â”‚                â”‚             â”‚            â”‚  Encoder)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Â¿QuÃ© es Query Rewriting?
**Problema**: "Â¿Y los permisos?" â†’ No tiene contexto suficiente

**SoluciÃ³n**: LLM reescribe la query incluyendo contexto
- Input: "Â¿Y los permisos?"
- Output: "Â¿CÃ³mo configurar permisos de acceso en AWS S3?"

**Comparamos**:
- âœ… **Con Rewrite**: Queries reescritas con contexto
- âŒ **Sin Rewrite**: Queries originales (solo Ãºltimo turno)

---

## ğŸ“ˆ SLIDE 3: Resultados - FASE 1 (Baselines)

### Tabla Comparativa: nDCG@10 por Dominio

| Modelo   | ClapNQ  | Cloud   | FiQA    | Govt    | **Promedio** |
|----------|---------|---------|---------|---------|--------------|
| BM25     | 0.266   | 0.238   | 0.127   | 0.302   | **0.233**    |
| BGE-M3   | 0.429   | 0.288   | 0.272   | 0.339   | **0.332**    |
| BGE-1.5  | 0.498   | 0.379   | 0.350   | 0.428   | **0.414**    |
| Voyage-3 | 0.522   | 0.355   | 0.311   | 0.446   | **0.408**    |
| **SPLADE** | **0.524** | **0.428** | **0.392** | **0.483** | **0.457** âœ¨ |

### ğŸ”‘ Hallazgos Clave

1. **SPLADE es el ganador claro**: Mejor en todos los dominios
   - +96% mejor que BM25 (baseline tradicional)
   - +38% mejor que BGE-M3
   - Consistente: no falla en ningÃºn dominio

2. **Voyage-3 es fuerte en ClapNQ**: Conversaciones generales
   - Mejor en ClapNQ (0.522), pero inconsistente
   - DÃ©bil en FiQA (-20% vs SPLADE)

3. **BGE-M3 decepciona**: A pesar de ser "multi-lingual + multi-modal"
   - Peor que BGE-1.5 en todos los dominios
   - No usar en configuraciones finales

### MÃ©tricas de Recall (Â¿Encontramos los documentos relevantes?)

| Modelo   | Recall@5 | Recall@10 | InterpretaciÃ³n |
|----------|----------|-----------|----------------|
| BM25     | 0.236    | 0.300     | Solo encuentra 30% de documentos relevantes |
| SPLADE   | 0.448    | 0.562     | Encuentra 56% - Â¡Mucho mejor! |
| BGE-1.5  | 0.428    | 0.526     | Similar a SPLADE |
| Voyage-3 | 0.407    | 0.529     | Bueno, pero no mejor que SPLADE |

---

## ğŸ”€ SLIDE 4: Resultados - FASE 2 (HÃ­bridos)

### Â¿Por quÃ© HÃ­bridos?
**Idea**: Combinar mÃ©todos sparse (lÃ©xico) + dense (semÃ¡ntico) para aprovechar fortalezas de ambos
- **Sparse (SPLADE)**: Bueno con tÃ©rminos exactos, acrÃ³nimos
- **Dense (BGE/Voyage)**: Bueno con sinÃ³nimos, parÃ¡frasis

### ConfiguraciÃ³n por Dominio

```
DOMINIOS FUERTES (ClapNQ, Govt):
  Retriever 1: SPLADE (sparse)     â†’ Top-300
  Retriever 2: Voyage-3 (dense)    â†’ Top-300
  FusiÃ³n: RRF (k=60)               â†’ Top-10

DOMINIOS DÃ‰BILES (Cloud, FiQA):
  Retriever 1: SPLADE (sparse)     â†’ Top-300
  Retriever 2: BGE-1.5 (dense)     â†’ Top-300
  FusiÃ³n: RRF (k=60)               â†’ Top-10
```

### Resultados: nDCG@10

| Dominio | Mejor Baseline | HÃ­brido | Mejora |
|---------|----------------|---------|--------|
| ClapNQ  | 0.524 (SPLADE) | **0.563** | +7.4% âœ… |
| Govt    | 0.483 (SPLADE) | **0.534** | +10.6% âœ… |
| Cloud   | 0.428 (SPLADE) | **0.440** | +2.8% âœ… |
| FiQA    | 0.392 (SPLADE) | **0.406** | +3.6% âœ… |

**Promedio General**: 0.486 (+6.3% vs mejor baseline)

### Recall@10: Â¿Mejora la cobertura?

| Dominio | Baseline | HÃ­brido | Mejora |
|---------|----------|---------|--------|
| ClapNQ  | 0.630    | **0.660** | +4.8% |
| Govt    | 0.611    | **0.646** | +5.7% |
| Cloud   | 0.522    | **0.544** | +4.2% |
| FiQA    | 0.487    | **0.505** | +3.7% |

**InterpretaciÃ³n**: Los hÃ­bridos encuentran mÃ¡s documentos relevantes que cualquier mÃ©todo individual.

---

## ğŸ¯ SLIDE 5: AblaciÃ³n - Â¿QuÃ© componente aporta mÃ¡s?

### Experimento de AblaciÃ³n

Comparamos **3 configuraciones** para entender el impacto de cada componente:

```
A. Baseline Individual:  SPLADE solo
B. HÃ­brido sin Rewrite:  SPLADE + Voyage/BGE (queries originales)
C. HÃ­brido con Rewrite:  SPLADE + Voyage/BGE (queries reescritas)
```

### Resultados de AblaciÃ³n: nDCG@10

| Config | ClapNQ | Govt | Cloud | FiQA | Promedio |
|--------|--------|------|-------|------|----------|
| A. SPLADE solo | 0.524 | 0.483 | 0.428 | 0.392 | 0.457 |
| B. HÃ­brido (No Rewrite) | 0.532 | 0.475 | 0.430 | 0.375 | 0.453 |
| C. HÃ­brido (Rewrite) | **0.563** | **0.534** | **0.440** | **0.406** | **0.486** |

### ğŸ“Š ContribuciÃ³n de Cada Componente

#### 1. Efecto de HÃ­brido (B vs A)
| Dominio | Cambio | InterpretaciÃ³n |
|---------|--------|----------------|
| ClapNQ  | +1.5% | PequeÃ±a mejora |
| Govt    | -1.7% | **Â¡Empeora!** |
| Cloud   | +0.5% | Mejora mÃ­nima |
| FiQA    | -4.3% | **Â¡Empeora!** |

**ConclusiÃ³n**: HÃ­brido sin rewrites **NO ayuda mucho** (a veces daÃ±a)

#### 2. Efecto de Query Rewrite (C vs B)
| Dominio | Cambio | InterpretaciÃ³n |
|---------|--------|----------------|
| ClapNQ  | +5.8% | **Â¡Gran mejora!** |
| Govt    | +12.4% | **Â¡Enorme mejora!** |
| Cloud   | +2.3% | Mejora moderada |
| FiQA    | +8.3% | **Â¡Gran mejora!** |

**ConclusiÃ³n**: Query Rewrite es **CRÃTICO** - aporta la mayor ganancia

#### 3. Efecto Combinado (C vs A)
```
HÃ­brido + Rewrite vs SPLADE solo:
  â†‘ +7.4% (ClapNQ)
  â†‘ +10.6% (Govt)
  â†‘ +2.8% (Cloud)
  â†‘ +3.6% (FiQA)
```

### ğŸ”‘ Mensaje Principal de AblaciÃ³n

**Ranking de Importancia**:
1. **Query Rewrite**: Componente mÃ¡s importante (+6-12% mejora)
2. **MÃ©todo Base (SPLADE)**: FundaciÃ³n sÃ³lida necesaria
3. **FusiÃ³n HÃ­brida**: Ayuda, pero solo con rewrites buenos

---

## ğŸ“Š SLIDE 6: Enfoque en MÃ©tricas Clave

### Â¿Por quÃ© nDCG@5 y nDCG@10?

**nDCG (Normalized Discounted Cumulative Gain)**: Mide quÃ© tan buenos son los resultados considerando:
1. **Relevancia**: Â¿Es relevante el documento?
2. **PosiciÃ³n**: Documentos mÃ¡s arriba valen mÃ¡s
3. **NormalizaciÃ³n**: Permite comparar entre queries

**@5 vs @10**: Miramos top-5 y top-10 porque:
- **@5**: Lo que el usuario ve sin scroll
- **@10**: Primera pÃ¡gina de resultados

### Resultados Finales: nDCG Comparado

#### nDCG@5 (Lo mÃ¡s crÃ­tico - sin scroll)

| Dominio | Baseline | HÃ­brido+Rewrite | Mejora |
|---------|----------|-----------------|--------|
| ClapNQ  | 0.468    | **0.517**       | +10.5% |
| Govt    | 0.422    | **0.491**       | +16.4% |
| Cloud   | 0.386    | **0.388**       | +0.5%  |
| FiQA    | 0.348    | **0.359**       | +3.2%  |

**Promedio**: 0.439 (baseline) â†’ **0.464** (hÃ­brido) = **+5.7% mejora**

#### nDCG@10 (Primera pÃ¡gina completa)

| Dominio | Baseline | HÃ­brido+Rewrite | Mejora |
|---------|----------|-----------------|--------|
| ClapNQ  | 0.524    | **0.563**       | +7.4%  |
| Govt    | 0.483    | **0.534**       | +10.6% |
| Cloud   | 0.428    | **0.440**       | +2.8%  |
| FiQA    | 0.392    | **0.406**       | +3.6%  |

**Promedio**: 0.457 (baseline) â†’ **0.486** (hÃ­brido) = **+6.3% mejora**

### Recall@5 y Recall@10 (Cobertura)

#### Recall@5
| Dominio | Baseline | HÃ­brido+Rewrite | Mejora |
|---------|----------|-----------------|--------|
| ClapNQ  | 0.498    | **0.557**       | +11.8% |
| Govt    | 0.470    | **0.546**       | +16.2% |
| Cloud   | 0.427    | **0.421**       | -1.4%  |
| FiQA    | 0.382    | **0.394**       | +3.1%  |

#### Recall@10
| Dominio | Baseline | HÃ­brido+Rewrite | Mejora |
|---------|----------|-----------------|--------|
| ClapNQ  | 0.630    | **0.660**       | +4.8%  |
| Govt    | 0.611    | **0.646**       | +5.7%  |
| Cloud   | 0.522    | **0.544**       | +4.2%  |
| FiQA    | 0.487    | **0.505**       | +3.7%  |

### ğŸ“‰ VisualizaciÃ³n Recomendada

Crear un grÃ¡fico de barras agrupadas:
```
        nDCG@5                    nDCG@10
    Baseline | HÃ­brido        Baseline | HÃ­brido
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ClapNQ  [====]  [======]      [=====]  [======]
Govt    [====]  [=======]     [====]   [======]
Cloud   [===]   [===]         [====]   [====]
FiQA    [===]   [===]         [===]    [====]
```

---

## ğŸ“ SLIDE 7: AnÃ¡lisis por Dominio

### Â¿Por quÃ© diferentes dominios tienen diferentes resultados?

#### 1. ClapNQ (Conversacional) - FUERTE
- **nDCG@10**: 0.563 (mejor)
- **CaracterÃ­stica**: Conversaciones naturales, contexto claro
- **Por quÃ© funciona bien**: Query rewriting captura bien el contexto conversacional

#### 2. Govt (Gubernamental) - FUERTE
- **nDCG@10**: 0.534 (segundo mejor)
- **CaracterÃ­stica**: Lenguaje formal, tÃ©rminos especÃ­ficos
- **Por quÃ© funciona bien**: SPLADE captura bien terminologÃ­a legal/tÃ©cnica

#### 3. Cloud (TÃ©cnico) - MODERADO
- **nDCG@10**: 0.440 (tercero)
- **CaracterÃ­stica**: DocumentaciÃ³n tÃ©cnica, acrÃ³nimos (AWS, Azure)
- **DesafÃ­o**: Muchos acrÃ³nimos y tÃ©rminos ambiguos

#### 4. FiQA (Financiero) - DIFÃCIL
- **nDCG@10**: 0.406 (mÃ¡s bajo)
- **CaracterÃ­stica**: Lenguaje tÃ©cnico financiero, cifras
- **DesafÃ­o**: Queries cortas, terminologÃ­a muy especÃ­fica

### Tabla Resumen por Dominio

| Dominio | Dificultad | Mejor Config | nDCG@10 | Recall@10 | Principal DesafÃ­o |
|---------|------------|--------------|---------|-----------|-------------------|
| ClapNQ  | Media | SPLADE+Voyage | 0.563 | 0.660 | Contexto conversacional |
| Govt    | Media | SPLADE+Voyage | 0.534 | 0.646 | TerminologÃ­a especÃ­fica |
| Cloud   | Alta | SPLADE+BGE15 | 0.440 | 0.544 | AcrÃ³nimos tÃ©cnicos |
| FiQA    | Muy Alta | SPLADE+BGE15 | 0.406 | 0.505 | Queries cortas |

---

## ğŸ”¬ SLIDE 8: FASE 3 - Reranking (Resultados Preliminares)

### Â¿QuÃ© es Reranking?
**Idea**: DespuÃ©s de retrieval inicial, usar un modelo mÃ¡s potente (cross-encoder) para reordenar los top-100 resultados

```
Retrieval HÃ­brido        Reranker             Final Top-10
(rÃ¡pido)                 (preciso)            (mejor calidad)

Top-300 docs    â†’    Top-100 docs    â†’    Top-10 docs
(SPLADE+Voyage)      (Re-score con        (Mejor orden)
                      cross-encoder)
```

### Resultados Actuales: Â¿Reranking Ayuda?

**IMPORTANTE**: Los archivos de reranking tienen las **mismas mÃ©tricas** que los hÃ­bridos, lo que sugiere:
1. El reranking no se ejecutÃ³ correctamente, O
2. Los archivos son copias de los hÃ­bridos

### MÃ©tricas (HÃ­brido vs Reranking)

| Dominio | HÃ­brido nDCG@10 | Rerank nDCG@10 | Diferencia |
|---------|-----------------|----------------|------------|
| ClapNQ  | 0.563           | 0.563          | **0.0%** âš ï¸ |
| Govt    | 0.534           | 0.534          | **0.0%** âš ï¸ |
| Cloud   | 0.440           | 0.440          | **0.0%** âš ï¸ |
| FiQA    | 0.406           | 0.406          | **0.0%** âš ï¸ |

### ğŸš¨ AcciÃ³n Requerida

**Para tu profesora, sÃ© transparente**:
- "Implementamos pipeline de reranking pero los resultados actuales son idÃ©nticos a hÃ­bridos"
- "Necesitamos verificar que el cross-encoder se ejecutÃ³ correctamente"
- "Expectativa teÃ³rica: +2-4% mejora basado en literatura"

---

## ğŸ“‹ SLIDE 9: Resumen Ejecutivo - Tabla Final

### ProgresiÃ³n de Resultados (nDCG@10)

| Etapa | MÃ©todo | ClapNQ | Cloud | FiQA | Govt | Promedio |
|-------|--------|--------|-------|------|------|----------|
| **Baseline** | BM25 | 0.266 | 0.238 | 0.127 | 0.302 | 0.233 |
| **Fase 1** | SPLADE | 0.524 | 0.428 | 0.392 | 0.483 | **0.457** |
| **Fase 2** | HÃ­brido+Rewrite | 0.563 | 0.440 | 0.406 | 0.534 | **0.486** |
| **Fase 3** | +Reranking | 0.563* | 0.440* | 0.406* | 0.534* | **0.486*** |

*Pendiente verificaciÃ³n

### Mejoras Totales vs Baseline Tradicional (BM25)

| MÃ©trica | BM25 | Mejor ConfiguraciÃ³n | Mejora Total |
|---------|------|---------------------|--------------|
| nDCG@5 | 0.203 | 0.464 | **+128%** ğŸš€ |
| nDCG@10 | 0.233 | 0.486 | **+109%** ğŸš€ |
| Recall@5 | 0.236 | 0.480 | **+103%** ğŸš€ |
| Recall@10 | 0.300 | 0.589 | **+96%** ğŸš€ |

---

## ğŸ¯ SLIDE 10: Conclusiones y Siguientes Pasos

### âœ… Conclusiones Principales

1. **SPLADE es el mejor retriever base**
   - Consistente en todos los dominios
   - +96% mejor que BM25 tradicional
   - FundaciÃ³n sÃ³lida para sistemas hÃ­bridos

2. **Query Rewriting es CRÃTICO**
   - Aporta el mayor impacto individual (+6-12%)
   - Necesario para contexto multi-turno
   - Sin rewrites, hÃ­bridos no funcionan bien

3. **HÃ­bridos mejoran sobre individuales**
   - +6.3% promedio con rewrites
   - Especialmente efectivo en ClapNQ y Govt (+7-11%)
   - Menos efectivo en dominios tÃ©cnicos (Cloud/FiQA)

4. **Diferencias significativas entre dominios**
   - Conversacional (ClapNQ): MÃ¡s fÃ¡cil (0.563)
   - Financiero (FiQA): MÃ¡s difÃ­cil (0.406)
   - Necesita configuraciÃ³n especÃ­fica por dominio

### ğŸ”§ Trabajo Pendiente

1. **Verificar pipeline de reranking**
   - Actualmente muestra mismos resultados que hÃ­bridos
   - Validar implementaciÃ³n del cross-encoder

2. **Optimizar para FiQA y Cloud**
   - Explorar query expansion especÃ­fica
   - Considerar embeddings especializados

3. **ValidaciÃ³n estadÃ­stica**
   - Tests de significancia (t-test pareado)
   - Intervalos de confianza

### ğŸ“Š ConfiguraciÃ³n Recomendada Final

```yaml
# ConfiguraciÃ³n Ã³ptima por dominio

ClapNQ:
  retrievers: [SPLADE, Voyage-3]
  query_rewrite: true
  fusion: RRF (k=60)
  top_k: 300 cada uno

Govt:
  retrievers: [SPLADE, Voyage-3]
  query_rewrite: true
  fusion: RRF (k=60)
  top_k: 300 cada uno

Cloud:
  retrievers: [SPLADE, BGE-1.5]
  query_rewrite: true
  fusion: RRF (k=60)
  top_k: 300 cada uno

FiQA:
  retrievers: [SPLADE, BGE-1.5]
  query_rewrite: true
  fusion: RRF (k=60)
  top_k: 300 cada uno
```

---

## ğŸ’¡ SLIDE EXTRA: Respondiendo Preguntas Comunes

### P1: Â¿Por quÃ© Recall@10 es menor que nDCG@10?
**R**: Son mÃ©tricas diferentes:
- **Recall@10**: % de documentos relevantes encontrados en top-10
- **nDCG@10**: Calidad considerando orden y relevancia gradual
- Ejemplo: Recall=0.50 significa encontramos 50% de docs relevantes

### P2: Â¿Por quÃ© BGE-M3 funciona peor que BGE-1.5?
**R**: HipÃ³tesis:
- BGE-M3 estÃ¡ optimizado para multilingÃ¼e/multimodal
- Nuestros datos son inglÃ©s puro
- BGE-1.5 es mÃ¡s especializado para inglÃ©s â†’ mejor rendimiento

### P3: Â¿QuÃ© es RRF y por quÃ© k=60?
**R**: Reciprocal Rank Fusion
- Combina rankings de mÃºltiples retrievers
- Formula: `score = sum(1/(k + rank_i))`
- k=60 es valor estÃ¡ndar en literatura (no muy sensible)

### P4: Â¿Por quÃ© 300 documentos iniciales?
**R**: Trade-off recall vs latencia:
- Top-100: Muy rÃ¡pido pero pierde recall
- Top-300: Balance Ã³ptimo
- Top-1000: Mayor recall pero mucho mÃ¡s lento

### P5: Â¿CÃ³mo sÃ© que no hay overfitting?
**R**: 
- Usamos splits train/val/test fijos
- MÃ©tricas reportadas en validation set
- No tocamos test set hasta evaluaciÃ³n final
- Conversaciones completas en mismo split (no leakage)

---

## ğŸ“š Referencias y Configuraciones

### Modelos Usados
- **SPLADE**: `naver/splade-cocondenser-ensembledistil`
- **BGE-1.5**: `BAAI/bge-base-en-v1.5`
- **BGE-M3**: `BAAI/bge-m3`
- **Voyage-3**: API de Voyage AI
- **Cross-Encoder**: `cross-encoder/ms-marco-MiniLM-L-6-v2`

### HiperparÃ¡metros Clave
```yaml
retrieval:
  top_k: 300
  batch_size: 32

fusion:
  method: RRF
  k: 60

reranking:
  model: cross-encoder
  top_k_initial: 100
  top_k_final: 10
  batch_size: 16

evaluation:
  metrics: [nDCG@5, nDCG@10, Recall@5, Recall@10, MAP]
  primary_metric: nDCG@10
```

### Reproducibilidad
- Random seed: 42 (fijo)
- Python: 3.11
- PyTorch: 2.0+
- CUDA: 11.8
- Hardware: NVIDIA A100 40GB

---

## ğŸ¨ Tips para la PresentaciÃ³n

### Visualizaciones Recomendadas

1. **GrÃ¡fico de Barras**: nDCG@10 por mÃ©todo y dominio
   - Eje X: Dominios (ClapNQ, Cloud, FiQA, Govt)
   - Eje Y: nDCG@10 (0.0 - 0.6)
   - Barras agrupadas: Baseline, SPLADE, HÃ­brido

2. **LÃ­nea de Progreso**: Mejora por fase
   - Eje X: Fases (BM25 â†’ SPLADE â†’ HÃ­brido â†’ Reranking)
   - Eje Y: nDCG@10 promedio
   - Mostrar progresiÃ³n clara

3. **Tabla de Calor**: AblaciÃ³n por componente
   - Filas: Dominios
   - Columnas: Configuraciones
   - Colores: Verde (mejora) a Rojo (empeora)

4. **GrÃ¡fico Radar**: Fortalezas por dominio
   - 4 ejes: Los 4 dominios
   - LÃ­neas: Diferentes mÃ©todos
   - Muestra consistencia de SPLADE

### Estructura de PresentaciÃ³n

**Tiempo Total: 15-20 minutos**

1. Contexto (2 min) â†’ Slide 1
2. MetodologÃ­a (3 min) â†’ Slide 2
3. Fase 1: Baselines (3 min) â†’ Slide 3
4. Fase 2: HÃ­bridos (3 min) â†’ Slide 4
5. AblaciÃ³n (4 min) â†’ Slide 5 + 6
6. AnÃ¡lisis por Dominio (2 min) â†’ Slide 7
7. Conclusiones (3 min) â†’ Slide 9 + 10

**Â¡Deja tiempo para preguntas!**

### Mensajes Clave para Enfatizar

1. **"Query rewriting es el componente mÃ¡s importante"** â† Repetir 2-3 veces
2. **"SPLADE es mÃ¡s consistente que embeddings densos"** â† Mostrar en todos los dominios
3. **"Los hÃ­bridos ayudan, pero necesitan buenos rewrites"** â† Mostrar ablaciÃ³n
4. **"Diferentes dominios necesitan configuraciones diferentes"** â† Justificar decisiones

### Lo que NO debes decir

âŒ "El reranking funciona igual que hÃ­brido" â†’ Mejor: "Pendiente validar reranking"
âŒ "BGE-M3 es malo" â†’ Mejor: "BGE-M3 no es Ã³ptimo para nuestro caso de uso inglÃ©s"
âŒ "Voyage es caro" â†’ Mejor: "Voyage ofrece mejor rendimiento en ciertos dominios"
âŒ TÃ©rminos complejos sin explicar (DCG, RRF, cross-encoder) â†’ Siempre dar contexto

---

## âœ… Checklist Pre-PresentaciÃ³n

- [ ] Validar que mÃ©tricas de reranking son correctas
- [ ] Preparar 1-2 ejemplos concretos de queries
- [ ] Verificar que todos los nÃºmeros coinciden entre slides
- [ ] Practicar explicar RRF en 30 segundos
- [ ] Tener backup de cÃ³mo calcular nDCG (por si preguntan)
- [ ] Lista de limitaciones del estudio (para preguntas)
- [ ] Conocer papers de SPLADE, BGE, Voyage (referencias)

---

**Nota Final**: Esta guÃ­a estÃ¡ diseÃ±ada para una presentaciÃ³n acadÃ©mica clara y convincente. Enfatiza simplicidad sobre tecnicismos, resultados sobre implementaciÃ³n, y conclusiones accionables sobre detalles. Â¡Buena suerte!
